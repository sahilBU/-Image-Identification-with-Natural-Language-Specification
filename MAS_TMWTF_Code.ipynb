{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ars/AI/project/Data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c15471aadb44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mimg_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#Normalizing data according to image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "#Prepare dataset for one folder\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "os.chdir('/Users/ars/AI/project/')\n",
    "os.chdir(os.path.join(os.getcwd(),'Data'))\n",
    "print(os.getcwd())\n",
    "\n",
    "fl = os.listdir(os.getcwd())\n",
    "#fl.remove('.DS_Store')\n",
    "\n",
    "for dirName in fl:\n",
    "    path='/Users/ars/AI/project/Data/'+dirName+'/img/'\n",
    "    list1=os.listdir(path)\n",
    "    list1.sort()\n",
    "    for img in list1:\n",
    "        dst= path+dirName[0:len(dirName)-2]+img[-10:]\n",
    "        src= path+img\n",
    "        #print(src,dst)\n",
    "        os.rename(src,dst)\n",
    "\n",
    "df =pd.DataFrame(columns = ['label','x','y','width','height','img_name'])\n",
    "\n",
    "#Combining dataset\n",
    "for i in range(len(fl)):\n",
    "    #Label\n",
    "    label = fl[i]\n",
    "    \n",
    "    #Bounding Box\n",
    "    bbox = pd.read_csv(os.getcwd()+'/'+label+'/groundtruth.txt', sep=\",\", header=None)\n",
    "    bbox.columns = ['x','y','width','height']\n",
    "    \n",
    "    #Images\n",
    "    img = os.listdir(os.path.join(os.getcwd(),label,'img'))\n",
    "    img.sort()\n",
    "    \n",
    "    #Datset for the folder\n",
    "    train = pd.DataFrame(columns = ['label','x','y','width','height'])\n",
    "    train[\"label\"] = pd.Series([label[0:len(fl[i])-2]]*bbox.shape[0])\n",
    "    \n",
    "    train.iloc[:,1:5] = bbox[:]\n",
    "    #print(train.shape)\n",
    "    img_folder = str(os.path.join(os.getcwd(),label,'img')) + '/'\n",
    "    \n",
    "    train[\"img_name\"] = img\n",
    "    \n",
    "    #Normalizing data according to image size\n",
    "    im = Image.open(os.path.join(os.getcwd(),label,'img',img[0]))\n",
    "    width, height = im.size\n",
    "    \n",
    "    train['x'] = train['x']/width\n",
    "    train['y'] = train['y']/height\n",
    "    train['width'] = train['width']/width\n",
    "    train['height'] = train['width']/height\n",
    "\n",
    "    #Combining dataset\n",
    "    df = df.append(train)\n",
    "    \n",
    "fl.sort()\n",
    "\n",
    "label_names = {}\n",
    "for i in range(len(fl)):\n",
    "    label_names[str(fl[i][0:len(fl[i])-2])] = i\n",
    "\n",
    "print(label_names)\n",
    "df_new = df[:]\n",
    "df_new[\"label\"].replace(label_names, inplace =True)\n",
    "\n",
    "print(df_new.iloc[0][5].split('.')[0]+'.txt')\n",
    "\n",
    "txt_dir='/Users/ars/AI/project/test/'\n",
    "\n",
    "os.chdir(txt_dir)\n",
    "\n",
    "for j in range(len(df_new)):\n",
    "    with open (df_new.iloc[j][5].split('.')[0]+'.txt','w') as f:\n",
    "        for i in range(0,5):\n",
    "            f.write(\"%s\" % df.iloc[j][i]+\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Training and testing data\n",
    "\n",
    "Command to run the code is \n",
    "\n",
    "python3 splitTrainAndTest.py /home/mohitbeniwal/project/JPEGImages\n",
    "\n",
    "This will split the images in the path into snowman_test.txt and snowman_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting data into test and train\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "imagesFolder='/Users/ars/AI/project/JPEGImages'\n",
    "def split_data_set(image_dir):\n",
    "\n",
    "    f_val = open(\"snowman_test.txt\", 'w')\n",
    "    f_train = open(\"snowman_train.txt\", 'w')\n",
    "    \n",
    "    path, dirs, files = next(os.walk(image_dir))\n",
    "    data_size = len(files)\n",
    "\n",
    "    ind = 0\n",
    "    data_test_size = int(0.1 * data_size)\n",
    "    test_array = random.sample(range(data_size), k=data_test_size)\n",
    "    \n",
    "    for f in os.listdir(image_dir):\n",
    "        if(f.split(\".\")[1] == \"jpg\"):\n",
    "            ind += 1\n",
    "            \n",
    "            if ind in test_array:\n",
    "                f_val.write(image_dir+'/'+f+'\\n')\n",
    "            else:\n",
    "                f_train.write(image_dir+'/'+f+'\\n')\n",
    "\n",
    "\n",
    "split_data_set(imagesFolder)\n",
    "print('Done splitting data into test and train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize Images script\n",
    "\n",
    "This takes the path for the folder with unedited original images\n",
    "Saves the resized files to a destination folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boatat0013_yolo_out_py_boat_detected.jpg\n",
      "boatat0013.jpg\n",
      "tigerer0014.jpg\n",
      "catat1756.jpg\n",
      "boatat0356.jpg\n",
      "boatat0013_yolo_out_py_surf_detected.jpg\n",
      "kitete1214.jpg\n",
      "surfboardrd0964.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "original_folder_name = '/Users/ars/AI/project/JPEGImages'\n",
    "resize_folder_name = '/Users/ars/AI/project/resize_images'\n",
    "\n",
    "content = os.listdir('/Users/ars/AI/project/JPEGImages')\n",
    "\n",
    "os.chdir(resize_folder_name)\n",
    "\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "img = []\n",
    "cnt = 0\n",
    "\n",
    "# for i in range(1):\n",
    "for i in range(len(content)):\n",
    "    img.append(content[i])\n",
    "\n",
    "desired_size = 416\n",
    "\n",
    "for filename in img:\n",
    "    im = cv2.imread(os.path.join(original_folder_name,filename))\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    print(filename)\n",
    "    cv2.imwrite(filename,new_im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Getting all the annotations of all images in the data set this includes all the annotations for 20 videos in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(r'/home/santoshganti/ai/Data')\n",
    "# txt_files_only_subdirs = path.glob('*/nlp.txt')\n",
    "txt_files_all_recursively = path.rglob('nlp*.txt') # including the current dir\n",
    "#category = txt_files_all_recursively.split('/')\n",
    "#print(category)\n",
    "\n",
    "for file in txt_files_all_recursively:\n",
    "    category = str(file).split('/')\n",
    "    #print(category[5])\n",
    "    contents = open(str(file),\"r\") \n",
    "    print(category[5][:-2],\" : \",contents.read())\n",
    "\n",
    "pd.DataFrame(,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Below is the code is plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lines = []\n",
    "for line in open(sys.argv[1]):\n",
    "    if \"avg\" in line:\n",
    "        lines.append(line)\n",
    "\n",
    "iterations = []\n",
    "avg_loss = []\n",
    "\n",
    "print('Retrieving data and plotting training loss graph...')\n",
    "for i in range(len(lines)):\n",
    "    lineParts = lines[i].split(',')\n",
    "    iterations.append(int(lineParts[0].split(':')[0]))\n",
    "    avg_loss.append(float(lineParts[1].split()[0]))\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(0, len(lines)):\n",
    "    plt.plot(iterations[i:i+2], avg_loss[i:i+2], 'r.-')\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Avg Loss')\n",
    "fig.savefig('training_loss_plot.png', dpi=1000)\n",
    "\n",
    "print('Done! Plot saved as training_loss_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Below is the code for Rnn Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 11 01:06:01 2019\n",
    "\n",
    "@author: mohitbeniwal\n",
    "\"\"\"\n",
    "from keras.models import load_model\n",
    "import os\n",
    "os.getcwd()\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "os.chdir('/home/santoshganti/ai/rnn')\n",
    "\n",
    "annotation = open(os.path.join('/home/santoshganti/ai/rnn','annotations.txt'))\n",
    "\n",
    "df = pd.DataFrame(columns = ['text','labels'])\n",
    "\n",
    "row_cnt = 0\n",
    "\n",
    "#Create dictionary for labels\n",
    "for i in annotation.readlines():\n",
    "\n",
    "    x = i.split('.txt  :  ')\n",
    "    \n",
    "    label_text = x[0].split('/')\n",
    "    label_text = label_text[len(label_text)-3]\n",
    "    \n",
    "    text = x[1]\n",
    "    text = text[0:len(text)-1]\n",
    "    \n",
    "    df = df.append({'labels':label_text,'text':text}, ignore_index = True)\n",
    "    \n",
    "    row_cnt += 1\n",
    "\n",
    "#Creating dictionary\n",
    "label_set = list(set(df['labels']))\n",
    "label_set.sort()\n",
    "\n",
    "labels_index = {}\n",
    "\n",
    "for i in range(len(label_set)):\n",
    "    labels_index[label_set[i]] = i\n",
    "\n",
    "#List of labels and texts\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "i = 0\n",
    "for i in range(df.shape[0]):\n",
    "    texts.append(df.text.iloc[i])\n",
    "    labels.append(labels_index[df.labels.iloc[i]])\n",
    "\n",
    "#Defining MAX_NB_length\n",
    "#Defining MAX_SEQUENCE_LENGTH\n",
    "unique_words = []\n",
    "max_l = 0\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    split_words = texts[i].split(' ')\n",
    "    unique_words.extend(split_words)\n",
    "    if(len(split_words) > max_l):\n",
    "        max_l = len(split_words)\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = len(set(unique_words))\n",
    "MAX_SEQUENCE_LENGTH = max_l\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "VALIDATION_SPLIT = 0.25\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "## Preparing embedding layer\n",
    "\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = os.path.join(os.getcwd(),'glove.6B')\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "## Preparing embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "## Loading embedding matrix into embedding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "        \n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "## Training Conv Net\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(128, 2, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(128, 2, activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=5)\n",
    "\n",
    "#New prediction\n",
    "new_text ='swimming in the water'\n",
    "new_text = new_text.split(' ')\n",
    "    \n",
    "sequences = tokenizer.texts_to_sequences(new_text)\n",
    "\n",
    "text_input = []\n",
    "for i in range(len(sequences)):\n",
    "    text_input.extend(sequences[i])\n",
    "\n",
    "text_input = [text_input]\n",
    "data = pad_sequences(text_input, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(data)\n",
    "\n",
    "np.argmax(model.predict(data))\n",
    "\n",
    "model.save('glove_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Below is the code for object detection once the training is done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is written at BigVision LLC. It is based on the OpenCV project. It is subject to the license terms in the LICENSE file found in this distribution and at http://opencv.org/license.html\n",
    "\n",
    "# Usage example:  python3 object_detection_yolo.py --video=run.mp4\n",
    "#                 python3 object_detection_yolo.py --image=bird.jpg\n",
    "\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pickle\n",
    "from keras.models import load_model \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the parameters\n",
    "#confThreshold = 0.5  #Confidence threshold\n",
    "#nmsThreshold = 0.4  # Non-maximum suppression threshold\n",
    "\n",
    "inpWidth = 416  #608     #Width of network's input image\n",
    "inpHeight = 416 #608     #Height of network's input image\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Object Detection using YOLO in OPENCV')\n",
    "parser.add_argument('--image', help='Path to image file.')\n",
    "parser.add_argument('--video', help='Path to video file.')\n",
    "args = parser.parse_args()\n",
    "        \n",
    "# Load names of classes\n",
    "classesFile = \"classes.names\";\n",
    "\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "\n",
    "modelConfiguration = \"/home/santoshganti/learnopencv/YOLOv3-Training-Snowman-Detector/darknet-yolov3.cfg\";\n",
    "modelWeights = \"/home/santoshganti/learnopencv/YOLOv3-Training-Snowman-Detector/weights/darknet-yolov3_final.weights\";\n",
    "\n",
    "net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "    # Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Draw the predicted bounding box\n",
    "def drawPred(classId, conf, left, top, right, bottom):\n",
    "    # Draw a bounding box.\n",
    "    #    cv.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
    "    cv.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 3)\n",
    "\n",
    "    label = '%.2f' % conf\n",
    "        \n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s:%s' % (classes[classId], label)\n",
    "\n",
    "    #Display the label at the top of the bounding box\n",
    "    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    top = max(top, labelSize[1])\n",
    "    cv.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (0, 0, 255), cv.FILLED)\n",
    "    #cv.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine),    (255, 255, 255), cv.FILLED)\n",
    "    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 2)\n",
    "\n",
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "def postprocess(frame, outs, object_class):\n",
    "    #print (\"outs :\", outs)\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    max_conf = 0\n",
    "    my_det = []\n",
    "    my_classId = 0\n",
    "    for out in outs:\n",
    "        #print(\"out.shape : \", out.shape)\n",
    "        for detection in out:\n",
    "            #if detection[4]>0.001:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            \n",
    "            #if scores[classId]>confThreshold:\n",
    "            confidence = scores[classId]\n",
    "            print(classId)\n",
    "            print(\"object class:\",object_class)\n",
    "            if (object_class==classId and confidence > max_conf):\n",
    "                #print(\"confidence \", confidence)\n",
    "                max_conf = confidence\n",
    "                my_det = detection \n",
    "                my_classId = classId\n",
    "    \n",
    "     #print(\"my_det \", my_det)\n",
    "    if len(my_det) == 0:\n",
    "        return [0, 0, 0, 0, 0]\n",
    "    center_x = int(my_det[0] * frameWidth)\n",
    "    center_y = int(my_det[1] * frameHeight)\n",
    "    width = int(my_det[2] * frameWidth)\n",
    "    height = int(my_det[3] * frameHeight)\n",
    "    left = int(center_x - width / 2)\n",
    "    top = int(center_y - height / 2)\n",
    "    classIds.append(my_classId)\n",
    "    confidences.append(float(max_conf))\n",
    "    boxes.append([left, top, width, height])\n",
    "    \n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    #indices = cv.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    \n",
    "    drawPred(classIds[0], confidences[0], left, top, left + width, top + height)\n",
    "    return[classIds[0], left, top, left + width, top + height]\n",
    "\n",
    "# Process inputs\n",
    "winName = 'Deep learning object detection in OpenCV'\n",
    "cv.namedWindow(winName, cv.WINDOW_NORMAL)\n",
    "outputFile = \"yolo_out_py.avi\"\n",
    "\n",
    "txtFile = open(args.image,'r')\n",
    "txt = txtFile.readlines()\n",
    "\n",
    "label_line = txt.pop(0)\n",
    "\n",
    "new_text = label_line.split(' ')\n",
    "\n",
    "with open('/home/santoshganti/ai/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(new_text)\n",
    "text_input = []\n",
    "for i in range(len(sequences)):\n",
    "    text_input.extend(sequences[i])\n",
    "\n",
    "text_input = [text_input]\n",
    "MAX_SEQUENCE_LENGTH=16\n",
    "data = pad_sequences(text_input, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "model = load_model('/home/santoshganti/ai/glovecnn.h5')\n",
    "object_class = np.argmax(model.predict(data))\n",
    "print(\"object\",object_class)\n",
    "path=args.image[:-4]\n",
    "list_main=[]\n",
    "count=0\n",
    "for img in txt:\n",
    "    img=img[:-1]\n",
    "    image_link = str(img)\n",
    "    image_name=image_link.split(\"/\")[-1][:-4]\n",
    "    \n",
    "    if (args.image):\n",
    "        # Open the image file\n",
    "        if not os.path.isfile(image_link):\n",
    "            print(\"Input image file \", image_link, \" doesn't exist\")\n",
    "            sys.exit(1)\n",
    "        cap = cv.VideoCapture(image_link)\n",
    "        outputFile = path+\"/temp/\"+image_name+'_yolo_out_py.jpg'\n",
    "    elif (args.video):\n",
    "        # Open the video file\n",
    "        if not os.path.isfile(args.video):\n",
    "            print(\"Input video file \", args.video, \" doesn't exist\")\n",
    "            sys.exit(1)\n",
    "        cap = cv.VideoCapture(args.video)\n",
    "        outputFile = args.video[:-4]+'_yolo_out_py.avi'\n",
    "    else:\n",
    "        # Webcam input\n",
    "        cap = cv.VideoCapture(0)\n",
    "\n",
    "    # Get the video writer initialized to save the output video\n",
    "    if (not args.image):\n",
    "        vid_writer = cv.VideoWriter(outputFile, cv.VideoWriter_fourcc('M','J','P','G'), 30, (round(cap.get(cv.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "    while cv.waitKey(1) < 0:\n",
    "\n",
    "        # get frame from the video\n",
    "        hasFrame, frame = cap.read()\n",
    "\n",
    "        # Stop the program if reached end of video\n",
    "        if not hasFrame:\n",
    "            #print(\"Done processing !!!\")\n",
    "            #print(\"Output file is stored as \", outputFile)\n",
    "            cv.waitKey(3000)\n",
    "            break\n",
    "\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(getOutputsNames(net))\n",
    "\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        list1 = postprocess(frame, outs,object_class)\n",
    "        list1.insert(0,img)\n",
    "        list_main.append(list1)\n",
    "        \n",
    "        if( len(list_main)  %10 == 0):\n",
    "            df = pd.DataFrame(list_main)\n",
    "            print(\"done\")\n",
    "            df.to_csv('out.csv')\n",
    "\n",
    "        # Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
    "        t, _ = net.getPerfProfile()\n",
    "        label = 'Inference time: %.2f ms' % (t * 1000.0 / cv.getTickFrequency())\n",
    "        #cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "        # Write the frame with the detection boxes\n",
    "        if (args.image):\n",
    "            cv.imwrite(outputFile, frame.astype(np.uint8));\n",
    "        else:\n",
    "            vid_writer.write(frame.astype(np.uint8))\n",
    "\n",
    "        cv.imshow(winName, frame)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list_main)\n",
    "print(\"done\")\n",
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Below is the code for getting map scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr 12 10:26:41 2019\n",
    "\n",
    "@author: mohitbeniwal\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('/home/santoshganti/learnopencv/YOLOv3-Training-Snowman-Detector/out.csv')\n",
    "\n",
    "#print(df.columns.values)\n",
    "\n",
    "df.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "df.columns = ['image_path','class_label','left','up','right','down']\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(columns = ['image_path','class_label','left','up','right','down'])\n",
    "ap = []\n",
    "\n",
    "def find_match(contents, x):\n",
    "    \n",
    "    obj = contents.split(' ')\n",
    "    \n",
    "    obj_class = str(obj[0])\n",
    "    obj_left = float(obj[1]) / 416.0\n",
    "    obj_up = float(obj[2]) / 416.0\n",
    "    obj_right = float(obj_up) + float(obj[3]) / 416.0\n",
    "    obj_down = obj_left + float(obj[4]) / 416.0\n",
    "    \n",
    "    x_min = max(x[2], obj_left)\n",
    "    x_max = min(x[2]+x[4], obj_right)\n",
    "    y_min = max(x[3], obj_up)\n",
    "    y_max = min(x[3]+x[5], obj_down)\n",
    "    \n",
    "    area = max(( x_max - x_min ) * ( y_max - y_min ) ,0)\n",
    "    actual = x[4] * x[5]\n",
    "    \n",
    "    if(actual ==0):\n",
    "        return 0\n",
    "    score = area/actual\n",
    "    \n",
    "\n",
    "    match_class = ( int(x[1]) ==int( obj_class) )*1\n",
    "\n",
    "    return(score*match_class)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    x = df.iloc[i]\n",
    "    file_name = x[0].split(\"/\")[-1][:-4]\n",
    "    file_name = str(file_name) + '.txt'\n",
    "    label_dir = '/home/santoshganti/learnopencv/YOLOv3-Training-Snowman-Detector/labels'\n",
    "    \n",
    "    f = open(os.path.join(label_dir, file_name))\n",
    "    contents = f.read()\n",
    "    \n",
    "    area = find_match(contents, x)\n",
    "    \n",
    "    ap.append(area)\n",
    "\n",
    "    \n",
    "ser = pd.Series(ap)\n",
    "print(ser.mean())\n",
    "ser.to_csv('map_scores.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
